ndu pre-check  for upgrade check

http://128.221.252.2/  - default mgmt ip...

 management server was able to communicate with the engines. Same was verified by pinging to their IPs (Director 1-A via 128.221.252.35 and Director 1-B via 128.221.252.36, 67 and 68 for second engine..


>>ll /management-server/ports/eth3
>>management-server set-ip -i IP_address/netmask -g gateway eth3 --> set ip if not exits
# sudo /opt/emc/VPlex/tools/ipconfig/changehostname.py -n hostname  --if required  

VPlexPlatformHealthCheck -> from shell prompt

configuration sync-time show -> to list ntp server
configuration sync-time –f –i IP_address_of_remote_NTP_server ->vplexcli command

configuration connect-local-directors -f

VPlexcli:version -a / Version --> details version of geo, mgmt, director...
VPlexcli:/notifications/call-home> ls
VPlexcli:/notifications/call-home> set enabled true
VPlexcli:/notifications/call-home> set enabled false/true --force (false to disable in upgrade situations.)
VPlexcli:/> health-check --full --verbose


export port summary -> to detail front end port status

configuration system-setup - start ez setup
ll /engines/**/ports -p --> to display ports  (do backend zoning)
or ll engines/engine-1-*/directors/*/hardware/ports
ll /engines/**/directors  - to list directors
ls -t /engines/**/directors*::director-id - to list seeds.


configuration continue-system-setup - continue EZsetup...

vplexcli -> configuration show-meta-volume-candidates -> anything larger than 78 G

meta-volume create –n c1_meta –d 
VPD83T3:60000970000192501423533036464234,
VPD83T3:6006016037202200d13e64035c1fe012

reverify the meta volume status using above show commands 
Cluster status  - shows cluster state...

configuration show-meta-volume-candidates -> to list rest of volumes

configuration metadata-backup  -- > configure time....

configuration complete-system-setup  -> complete 

configuration register-product - .to register product..

>>configuration enable-front-end-ports  -> enables front end port..

#health check   --> to show heallth status
ll /**/system-volumes -> list meta volumes
ll /**/storage-volume  (also storage-volume summary -> will give a summary of vol)
ll /**/extents
ll /**/devices
ll /**/virtual-volumes

usr creation :  user add TestUser ->only by admin .(will ask admin pwd)
if admin pwd lost call remote support for reset .  (else user reset -u <user name>) 
no shell access to vplexuser and readonly(geo 6.0)
security-admin role is assinged to admin accounts..

(belo commands available in vs6)
ll /management-server/users/local -> to list users
ls /management-server/users/local/<user> to list role details..
ls /management-server/users/local/<user> set role-name readonly -> available with readonly and vplexuser.
ls /management-server/users/local/<user> set shell-access=true -> available with readonly and vplexuser
 

Edit the login-banner

edit the file from 
vplexcli>> security set-login-banner -b /home/service/login-banner.txt
  security remove-login-banner  -> removing

set call-home manually (primus pdf doc is saved in computer)

VPlexcli:/notifications/call-home> test  
call-home test was successful.

also see https://community.emc.com/docs/DOC-42829


Ops steps

>>cd /clusters/cluster-Cluster_ID/storage-elements/storage-volumes
ls
>>storage-volume claim -n storage_volume_name -d storage_volume_ID
>>extent create -d storage_volume_name
>>ls /clusters/cluster-Cluster_ID/storage-elements/extents
>>cd /clusters/cluster-Cluster_ID/devices
>>local-device create device_name -g[raid-0|raid-1|raid-c] -e extent,extent
cd /clusters/cluster-Cluster_ID/virtual-volumes
create -r device_name
ll /clusters/cluster-Cluster_ID/exports/initiator-ports

Do masking and ig initiator ..

cd /clusters/cluster-Cluster_ID/exports/storage-views
export storage-view summary


connectivity validate-be --verbose - to verify multipath ...

To revert all setup...

configuration system-reset

ldap __> authetication directory-service show -->  to list 

SMTP server verify 

Service @ManagementServer:/> cat /opt/emc/connectemc/ConnectEMC_config.xml (https://community.emc.com/thread/137797?tstart=0)



snmp trap 

  * For PULLing info from VPLEX (snmp GET, not necessary for traps):
 
        snmp-agent configure                     # MUST be run as service
        snmp-agent start
        snmp-agent status
        snmp-agent unconfigure
 
        NOTE: MIB is located on management server: /opt/emc/VPlex/mibs/VPLEX-MIB.mib
 
 
  * For VPLEX SNMP traps generated by call-home  events:  (default port is 162)
 
        cd /notifications/call-home
        snmp-trap create  <trap-name>
        cd /notifications/call-home/snmp-traps/<trap_name>
        set remote-host  <IP Address>   # IP address of server receiving traps
        set community-string  <string>  # default is public
        set started true
        notifications call-home test will generate a test trap


####Vplex mgmt###########

Virtual volume expansion 

Storage volume expansion(available only for 1-1) and concatenate expansion (raid C)
ensure no Migration or upgrade is taking place..

command from cli - for storage and then vvol expansion , u can use gui also
virtualvolume> ll <vvol> -> shows expandable size
virtualvolume>  expand -v <vvol> -> expands vvol.

concatenate..

create the deice and 
run expand -v <vv> -e <device name>

adding a mirror.. use 'add local mirror' option




RP

rp validate-configuration

sessions - to list logged in users..

to find a volume if belongs to a storage view 
export storage-view find -v "vV name"
export storage-view find -i "initiator name" ->(supports wild card)

export storage-view addvirtualvolume --view Phy_nrusca-clp10002_Shared --virtual-volumes (53,nrusca-clp10002_u02) -f
export storage-view addvirtualvolume --view Phy_nrusca-clp10003_Shared --virtual-volumes (53,nrusca-clp10003_u02) -f


storage-volume find-array *de11
storage-tool compose -d VNX-2_L1982 -g raid-0 -n nrusca-clp10002_u02








